{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to NLP\n",
        "\n",
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/TheAIDojo/AI_4_Climate_Bootcamp/blob/main/Week 04 - Introduction to Sequence Modelling/1. Introduction to NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->\n",
        "\n",
        "Natural Language Processing (NLP) is a field that focuses on the interaction between computers and human language. It deals with the analysis, understanding and generation of human language in a meaningful and useful manner.\n",
        "\n",
        "One of the applications of NLP is sentiment analysis, which is the task of determining whether a given piece of text has a positive, negative or neutral sentiment. Another application is machine translation, which is the task of translating one language into another.\n",
        "\n",
        "TensorFlow and Keras are popular open-source tools used in NLP, which are both highly customizable and flexible. These tools allow you to build and train deep learning models for NLP tasks such as sentiment analysis, text classification and text generation.\n",
        "\n",
        "Pandas, Matplotlib and Numpy are data analysis libraries that are also commonly used in NLP. They allow you to manipulate and visualize data in a meaningful and useful manner.\n",
        "\n",
        "For further reading on NLP and its applications, check out the following resources:\n",
        "- [TensorFlow NLP Tutorials](https://www.tensorflow.org/tutorials/text)\n",
        "- [Keras NLP Guide](https://keras.io/guides/keras_nlp/getting_started/)\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents <a name=\"toc\"></a>\n",
        "- [Text Preprocessing](#text-preprocessing)\n",
        "  - [Text Cleaning](#text-cleaning)\n",
        "  - [Text Tokenization](#text-tokenization)\n",
        "  - [Text Padding](#text-padding)\n",
        "- [Dense Model Training](#dense-model)\n",
        "- [Word Embeddings](#word-embeddings)\n",
        "  - [Embedding Model](#embedding-model)\n",
        "- [Recurrent Neural Networks (RNNs)](#rnn)\n",
        "  - [Simple RNN Model](#simple-rnn-model)\n",
        "  - [LSTM Model](#lstm-model)\n",
        "  - [GRU Model](#gru-model)\n",
        "  - [Bidirectional RNN Model](#bidirectional-rnn-model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libaries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn import model_selection\n",
        "from nltk.corpus import (\n",
        "    stopwords,\n",
        ")  # stopwords module from NLTK (Natural Language Toolkit)\n",
        "import re  # built-in regular expression module\n",
        "import string  # built-in string module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download stopwords\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download Dataset from Kaggle\n",
        "\n",
        "Before running the celll below, make sure you upload your Kaggle API token to the notebook. You can do this by clicking on the \"Files\" tab on the left, then clicking on the \"Upload\" button. You can find your Kaggle API token by going to your Kaggle account, clicking on \"My Account\", then clicking on \"Create New API Token\". You can then upload the \"kaggle.json\" file to the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upload kaggle.json into this folder before running this command\n",
        "!mkdir /root/.kaggle\n",
        "!cp kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read dataset from csv file\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing <a name=\"text-preprocessing\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Before training our NLP models, it is necessary to preprocess the text data so that it is in a format that can be easily understood by the models. There are several steps involved in text preprocessing, including text cleanup, tokenization, and padding.\n",
        "\n",
        "- Text cleanup is the process of removing any unwanted elements from the text, such as punctuation, stop words, numbers, and special characters. This step is important because these elements can often introduce noise into the data and negatively impact the performance of the model. Some common text cleanup tasks include:\n",
        "  - Removing punctuation\n",
        "  - Removing stop words\n",
        "  - Removing numbers\n",
        "  - Removing special characters\n",
        "  - Lowercasing the text\n",
        "\n",
        "- Tokenization is the process of converting a piece of text into individual tokens, which are typically words or phrases. This step is important because it allows the model to work with individual words or phrases rather than with the entire text.\n",
        "\n",
        "- Padding is the process of adding padding tokens to the text so that all the text sequences have the same length. This step is important because many NLP models require fixed input lengths.\n",
        "\n",
        "Let's go through them step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first, we will set some parameters\n",
        "vocab_size = 8000  # number of words in the vocabulary, we will use the top 8000 most common words\n",
        "max_length = 120  # maximum length of a review, we will truncate reviews longer than 120 words and pad reviews shorter than 120 words\n",
        "embedding_dim = 50  # dimension of the embedding vector, we will use 50-dimensional embedding vectors\n",
        "batch_size = 32  # number of reviews in each batch\n",
        "seed = 42  # random seed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Cleanup <a name=\"text-cleanup\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Text cleanup is an important step in text preprocessing that removes any unwanted elements from the text, such as punctuation, stop words, numbers, and special characters. The motivation behind text cleanup is to reduce noise in the data and improve the performance of the model.\n",
        "\n",
        "There are several tasks involved in text cleanup, including:\n",
        "- Removing punctuation\n",
        "- Removing stop words\n",
        "- Removing numbers\n",
        "- Removing special characters\n",
        "- Lowercasing the text\n",
        "  \n",
        "\n",
        "It is important to consider which text cleanup tasks are necessary for your specific use case. For example, if you are performing sentiment analysis, it may not be necessary to remove numbers from the text. On the other hand, if you are performing machine translation, it may be necessary to remove special characters from the text.\n",
        "\n",
        "In the next cells, we will show you how to perform text cleanup in Python using TensorFlow and Keras.\n",
        "\n",
        "For comprehensive text cleanup functions, check out the following code:\n",
        "- [English Text Cleanup](https://github.com/jfilter/clean-text/blob/main/cleantext/clean.py)\n",
        "- [Arabic Text Cleanup](https://github.com/ARBML/tnkeeh/blob/master/tnkeeh/tnkeeh.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production the filming tech...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically there s a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei s love in the time of money is a...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  one of the other reviewers has mentioned that ...  positive\n",
              "1  a wonderful little production the filming tech...  positive\n",
              "2  i thought this was a wonderful way to spend ti...  positive\n",
              "3  basically there s a family where a little boy ...  negative\n",
              "4  petter mattei s love in the time of money is a...  positive"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def text_cleanup(text, remove_stopwords=False):\n",
        "    # change all text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove HTML tags\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "\n",
        "    # remove numbers\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "    # remove words with numbers\n",
        "    text = re.sub(r\"\\w*\\d\\w*\", \"\", text)\n",
        "\n",
        "    # remove URLs\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "    # remove emails\n",
        "    text = re.sub(r\"\\S*@\\S*\\s?\", \"\", text)\n",
        "\n",
        "    # remove mentions (@username)\n",
        "    text = re.sub(r\"@\\S+\", \"\", text)\n",
        "\n",
        "    # remove hashtags (#)\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)\n",
        "\n",
        "    # remove Punctuation\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "\n",
        "    # remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # note that removing stopwords is optional and depends on the task, in some tasks removing stopwords can be beneficial, in other tasks removing stopwords can be harmful.\n",
        "    # as a rule of thumb, you should try both options and see which one works better for your task.\n",
        "\n",
        "    if remove_stopwords:\n",
        "        # remove stopwords (the, a, an, etc.)\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        words = word_tokenize(text)\n",
        "        words = [word for word in words if not word in stop_words]\n",
        "        text = \" \".join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# apply text_cleanup function to the reviews\n",
        "df[\"review\"] = df[\"review\"].map(text_cleanup)\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Tokenization <a name=\"tokenization\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "Text tokenization is the process of converting a sentence or document into tokens, or meaningful units of words. This is an important step in preparing text data for training machine learning models.\n",
        "\n",
        "In Keras and TensorFlow, text tokenization can be performed using the Tokenizer class. This class has several important parameters:\n",
        "\n",
        "- `num_words`: The maximum number of words to keep, based on word frequency. Only the most common `num_words` will be kept, and all other words will be set to an Out-of-Vocabulary (OOV) token.\n",
        "- `oov_token`: The string that will be used to represent OOV words.\n",
        "- `filters`: A string of characters to filter out, for example punctuation.\n",
        "- `lower`: A flag to convert all text to lowercase before tokenization.\n",
        "\n",
        "Let's see how to use the Tokenizer class to tokenize text in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'the': 2, 'and': 3, 'a': 4, 'of': 5, 'to': 6, 'is': 7, 'it': 8, 'in': 9, 'i': 10}\n"
          ]
        }
      ],
      "source": [
        "# create tokenizer object\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=vocab_size, oov_token=\"<OOV>\"\n",
        ")\n",
        "\n",
        "# fit tokenizer on the reviews\n",
        "tokenizer.fit_on_texts(df[\"review\"])\n",
        "\n",
        "# preview the word index, notice that the word index is sorted by frequency\n",
        "word_index = tokenizer.word_index\n",
        "print({k: word_index[k] for k in list(word_index)[:10]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[11, 7, 4, 1, 3024, 8, 1372, 49, 663, 12, 26, 2596, 3, 49, 663, 12, 26, 24, 2596]]\n"
          ]
        }
      ],
      "source": [
        "# let's see how the tokenizer works\n",
        "text = \"This is a sample text, it contains some words that are repeated, and some words that are not repeated.\"\n",
        "sequence = tokenizer.texts_to_sequences([text])\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[29, 5, 2, 78, 2040, 47, 1051, 12, 101, 150, 42, 3068, 395, 21, 231, 30, 3173, 33, 26, 204, 15, 11, 7, 615, 48, 591, 18, 69, 2, 88, 149, 12, 3217, 69, 45, 3068, 14, 92, 5323, 3, 1, 136, 5, 561, 62, 266, 9, 204, 38, 2, 649, 142, 1720, 69, 11, 7, 24, 4, 117, 17, 2, 7805, 2309, 41, 1, 11, 117, 2572, 57, 5843, 18, 5465, 6, 1454, 372, 41, 561, 92, 7, 3784, 9, 2, 356, 357, 5, 2, 649, 8, 7, 433, 3068, 15, 12, 7, 2, 1, 358, 6, 2, 1, 6785, 2543, 1032, 1, 8, 2683, 1398, 23, 1, 520, 35, 4638, 2437, 5, 2, 1182, 116, 31, 2, 6924, 28, 2884, 1, 3, 386, 1, 37, 1, 7, 24, 298, 23, 2, 4848, 2914, 520, 7, 342, 6, 108, 1, 1, 1, 1, 4989, 7684, 2425, 3, 53, 37, 1, 325, 1, 7229, 1, 3, 1, 1, 26, 112, 224, 241, 10, 61, 133, 2, 281, 1312, 5, 2, 117, 7, 682, 6, 2, 193, 12, 8, 267, 116, 78, 274, 574, 22, 2993, 819, 183, 1289, 4125, 17, 2473, 1211, 819, 1419, 819, 865, 3068, 153, 22, 940, 185, 2, 88, 395, 10, 124, 210, 3217, 69, 15, 37, 1604, 8, 14, 2220, 10, 412, 22, 133, 10, 14, 1567, 17, 8, 19, 15, 10, 291, 53, 10, 1402, 4, 1254, 17, 3068, 3, 191, 1, 6, 2, 298, 2019, 5, 2115, 561, 24, 42, 561, 19, 7568, 7070, 4957, 36, 231, 30, 2956, 44, 17, 4, 1, 6786, 36, 231, 496, 23, 630, 3, 76, 241, 18, 8, 70, 7510, 640, 695, 6786, 110, 650, 84, 1182, 1, 682, 6, 67, 566, 5, 891, 2001, 41, 1182, 551, 150, 3068, 21, 198, 426, 3802, 18, 48, 7, 3287, 795, 1581, 46, 21, 51, 76, 9, 1200, 18, 127, 4059, 480]\n",
            "[4, 391, 121, 351, 2, 1362, 2950, 7, 54, 1, 54, 159, 58, 2145, 1568, 3, 410, 4, 1, 3, 527, 1, 282, 5, 1817, 6, 2, 440, 411, 2, 151, 26, 559, 70, 2251, 482, 4126, 24, 63, 47, 191, 31, 2, 1, 19, 25, 47, 31, 2, 2263, 177, 3249, 98, 21, 51, 368, 66, 2, 1, 779, 1, 34, 2, 1820, 6, 1713, 7276, 6508, 24, 63, 7, 8, 70, 277, 2, 150, 19, 8, 7, 4, 1, 405, 3, 2372, 411, 4, 4292, 351, 45, 29, 5, 2, 81, 1130, 13, 5, 201, 3, 27, 114, 2, 1817, 65, 269, 342, 18, 2, 121, 180, 2, 1007, 5, 2, 2874, 62, 249, 73, 357, 2, 2179, 953, 3102, 1268, 1174, 94, 4874, 8, 299, 23, 257, 1815, 3, 257, 4511, 575, 18, 2, 136, 3665, 1, 3, 1, 3, 2, 719, 575, 5, 67, 1036, 18, 1, 13, 1, 1, 172, 2270, 26, 1952, 70, 220]\n",
            "[10, 192, 11, 14, 4, 391, 96, 6, 1138, 58, 23, 4, 98, 857, 1452, 2573, 1223, 9, 2, 885, 1, 758, 3, 150, 4, 635, 2309, 201, 2, 113, 7, 4060, 19, 2, 409, 7, 1890, 3, 2, 102, 26, 1468, 60, 2, 70, 6004, 6596, 1538, 471, 137, 49, 198, 30, 669, 52, 33, 946, 11, 7, 24, 1013, 219, 2893, 5100, 10, 192, 8, 14, 3017, 12, 2811, 1768, 7, 132, 1385, 9, 1117, 5, 2, 394, 108, 5, 179, 28, 2041, 6, 111, 11, 14, 2, 90, 10, 223, 1426, 32, 29, 5, 2811, 13, 1292, 9, 155, 2993, 10, 133, 4, 2042, 137, 10, 138, 112, 77, 1489, 18, 1, 1, 9, 11, 55, 1310, 6, 1238, 177, 43, 1231, 1405, 3, 5030, 204, 84, 4, 831, 19, 3516, 187, 246, 11, 198, 24, 30, 2, 7119, 5051, 5, 27, 616, 19, 8, 14, 1, 73, 1798, 2908, 1, 3, 53, 218, 73, 2486, 4, 81, 201, 6, 142, 66, 18, 349]\n",
            "[678, 40, 13, 4, 227, 116, 4, 121, 398, 3319, 1234, 40, 13, 4, 984, 9, 27, 4178, 27, 759, 26, 982, 31, 2, 58, 11, 16, 7, 7743, 73, 4, 1812, 1279, 3, 1095, 3319, 1072, 6, 426, 5409, 3, 496, 2, 984, 568, 88, 5, 31, 52, 21, 152, 168, 6, 95, 4, 20, 21, 203, 1148, 46, 92, 4, 689, 41, 4, 444, 15, 4, 444, 2, 16, 7, 1688, 759, 26, 1, 6819, 39, 9, 146, 114, 3, 94, 71, 28, 3319, 18, 27, 4178, 62, 453, 4073, 31, 2, 20, 10, 846, 6, 66, 4, 1, 731, 16, 3, 301, 10, 291, 4, 444, 18, 49, 3834, 689, 3250, 44, 5, 42, 17, 2, 70, 393, 759, 4595, 3279, 15, 17, 2, 633, 18, 3319, 42, 2748, 93]\n",
            "[1, 1, 13, 111, 9, 2, 58, 5, 290, 7, 4, 2116, 1407, 20, 6, 104, 436, 1, 1549, 179, 4, 4249, 3280, 45, 390, 4432, 11, 7, 4, 16, 12, 186, 6, 30, 979, 179, 48, 290, 639, 3, 988, 79, 6, 80, 9, 2, 278, 1123, 71, 2462, 11, 110, 4, 7016, 23, 2, 1754, 1, 13, 286, 45, 2, 169, 757, 2, 154, 1, 2, 205, 6, 2, 978, 58, 170, 833, 116, 31, 134, 278, 102, 887, 3, 3742, 252, 29, 7, 3230, 9, 29, 96, 41, 160, 6, 2, 364, 388, 19, 57, 29, 186, 6, 120, 2, 916, 219, 5, 2875, 1, 2, 20, 47, 4, 3357, 1, 166, 71, 26, 599, 6, 66, 86, 134, 80, 418, 3, 2, 176, 33, 418, 9, 67, 199, 1, 2, 63, 149, 29, 217, 44, 5, 31, 134, 3363, 9, 2, 432, 7, 2, 278, 5725, 5, 4746, 252, 29, 1, 4, 190, 520, 7, 24, 615, 2, 119, 273, 9, 62, 390, 4432, 167, 4463, 1, 15, 29, 1, 7, 2, 414, 18, 90, 5, 2, 80, 71, 2462, 2, 115, 7, 50, 457, 436, 1, 13, 458, 1284, 1, 1, 5101, 3571, 2666, 482, 1, 6977, 1, 3, 2, 367, 5, 2, 1009, 178, 95, 134, 102, 214, 1099, 71, 629, 436, 1, 50, 2015, 3, 1, 1, 17, 27, 364, 161]\n"
          ]
        }
      ],
      "source": [
        "# now let's convert the reviews to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df[\"review\"])\n",
        "\n",
        "# preview some sequences, notice that the sequences have different lengths\n",
        "for i in range(5):\n",
        "    print(sequences[i])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padding in NLP\n",
        "\n",
        "In NLP, it is common to have sequences of different lengths, for example, in a sentiment analysis task, you may have reviews of different lengths. However, deep learning models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) require input data to have the same length. Therefore, padding is used to make the sequences have the same length.\n",
        "\n",
        "In TensorFlow and Keras, padding can be done using the `pad_sequences` function from the `keras.preprocessing.sequence` module. The important parameters for this function are:\n",
        "\n",
        "- `sequences`: A list of sequences, where each sequence is a list of integers.\n",
        "- `maxlen`: The maximum length of the sequences. If the length of a sequence is greater than `maxlen`, it will be truncated, and if it is smaller, it will be padded with zeros.\n",
        "- `padding`: The type of padding to use, either `'pre'` or `'post'`. Default is `'pre'`.\n",
        "- `truncating`: The type of truncating to use, either `'pre'` or `'post'`. Default is `'pre'`.\n",
        "\n",
        "Here's an example of how to use the `pad_sequences` function:\n",
        "\n",
        "```python\n",
        "sequences = [[1, 2, 3, 4, 5, 6], [4, 5], [6]]\n",
        "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=5, padding='post')\n",
        "print(padded_sequences)\n",
        "```\n",
        "\n",
        "The output will be\n",
        "\n",
        "```\n",
        "[[1 2 3 4 5]\n",
        " [4 5 0 0 0]\n",
        " [6 0 0 0 0]]\n",
        "```\n",
        "\n",
        "In this example, all sequences have been padded to have a length of 5, and the padding was done on the right side (post).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded Sequences Shape:  (50000, 120)\n"
          ]
        }
      ],
      "source": [
        "# pad the sequences\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences, maxlen=max_length, padding=\"post\"\n",
        ")\n",
        "\n",
        "# print padded_sequences shape, notice that the shape is uniform (batch_size, max_length)\n",
        "print(\"Padded Sequences Shape: \", padded_sequences.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard Preprocessing Steps\n",
        "\n",
        "The following preprocessing steps are common to other classification tasks, we'll encoder the labels and split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape:  (45000, 120)\n",
            "x_test shape:  (5000, 120)\n",
            "y_train shape:  (45000,)\n",
            "y_test shape:  (5000,)\n"
          ]
        }
      ],
      "source": [
        "# store padded sequences to `x`\n",
        "x = padded_sequences\n",
        "\n",
        "# encode labels to y\n",
        "y = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
        "\n",
        "\n",
        "# split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
        "    x, y, test_size=0.1, random_state=seed, stratify=y\n",
        ")\n",
        "\n",
        "print(\"x_train shape: \", x_train.shape)\n",
        "print(\"x_test shape: \", x_test.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"y_test shape: \", y_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dense Model Training <a name=\"dense-model\"></a>\n",
        "[Back to Top](#toc)\n",
        "\n",
        "We will train a simple feed-forward (Dense) neural network model to classify the text data. This model will have an input layer, a hidden layer, and an output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M2 Pro\n",
            "\n",
            "systemMemory: 32.00 GB\n",
            "maxCacheSize: 10.67 GB\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 19:45:47.191124: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-02-04 19:45:47.191301: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                7744      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,857\n",
            "Trainable params: 9,857\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "dense_model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(max_length,)),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dense_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 19:46:12.419406: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
            "2023-02-04 19:46:12.567668: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - ETA: 0s - loss: 17.3928 - accuracy: 0.5022"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-04 19:46:21.867471: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1407/1407 [==============================] - 10s 6ms/step - loss: 17.3928 - accuracy: 0.5022 - val_loss: 0.8066 - val_accuracy: 0.5046\n",
            "Epoch 2/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7126 - accuracy: 0.5026 - val_loss: 0.7659 - val_accuracy: 0.4958\n",
            "Epoch 3/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6947 - accuracy: 0.5013 - val_loss: 0.7647 - val_accuracy: 0.5004\n",
            "Epoch 4/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6934 - accuracy: 0.5002 - val_loss: 0.7604 - val_accuracy: 0.5022\n",
            "Epoch 5/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6936 - accuracy: 0.5014 - val_loss: 0.7472 - val_accuracy: 0.5008\n",
            "Epoch 6/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6935 - accuracy: 0.5047 - val_loss: 0.7500 - val_accuracy: 0.4992\n",
            "Epoch 7/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6929 - accuracy: 0.4933 - val_loss: 0.7513 - val_accuracy: 0.4996\n",
            "Epoch 8/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6932 - accuracy: 0.4976 - val_loss: 0.6966 - val_accuracy: 0.4994\n",
            "Epoch 9/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6936 - accuracy: 0.4993 - val_loss: 0.7005 - val_accuracy: 0.5004\n",
            "Epoch 10/10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.6937 - accuracy: 0.4977 - val_loss: 0.7032 - val_accuracy: 0.4994\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x331f435e0>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compile the model\n",
        "dense_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# train the model\n",
        "dense_model.fit(\n",
        "    x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When using a dense only