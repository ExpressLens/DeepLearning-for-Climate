{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training Loops\n",
    "\n",
    "Custom training loops are a powerful tool for training deep learning models. They are used when the standard `fit` method provided by `Keras` or other high-level libraries is not enough to achieve the desired behavior.\n",
    "\n",
    "Custom training loops provide full control over the training process, including the forward and backward pass, the optimization step, and the calculation of metrics. This allows for the implementation of complex training procedures, such as multi-stage training, transfer learning, or custom regularization methods.\n",
    "\n",
    "The steps to create a custom training loop after preparing the data and defining the model are:\n",
    "\n",
    "1. Define essential variables like the optimizer, loss function, and metrics that will be used to evaluate the model's performance.\n",
    "2. Create a training step function that will be used to perform a single training step. This function will be called for each batch of data in the training set. The training step function should perform the following steps:\n",
    "   - Calculate the forward pass passing the input data to the model and calculating the predictions.\n",
    "   - Calculate the loss using the predictions and the true labels.\n",
    "   - Calculate the gradients using the loss and the model's variables.\n",
    "   - Apply parameters update using the optimizer and the gradients.\n",
    "   - Calculate the metrics using the predictions and the true labels.\n",
    "3. Create a test step function that will be used to perform a single test step. This function will be called for each batch of data in the test set. The test step function should perform the following steps:\n",
    "   - Calculate the forward pass passing the input data to the model and calculating the predictions.\n",
    "   - Calculate the loss using the predictions and the true labels.\n",
    "   - Calculate the metrics using the predictions and the true labels.\n",
    "4. Create a training loop that will be used to perform the training and test steps for each epoch. The training loop should perform the following steps:\n",
    "   - Iterate over the training set and call the training step function for each batch.\n",
    "   - Iterate over the test set and call the test step function for each batch.\n",
    "   - Print the loss and metrics for the current epoch.\n",
    "5. Profit!\n",
    "\n",
    "By using custom training loops, you can achieve better control and flexibility over the training process and achieve better results for your specific use case.\n",
    "\n",
    "## Table of Contents\n",
    "- [Dataset Preparation](#dataset-preparation)\n",
    "- [Model Definition](#model-definition)\n",
    "- [Custom Training Loop](#custom-training-loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds  # datasets\n",
    "from tqdm.notebook import tqdm  # progress bar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation <a name=\"dataset-preparation\"></a>\n",
    "[Back to Top](#toc)\n",
    "\n",
    "We'll use the same dataset setup as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load(\n",
    "    \"fashion_mnist\",  # name of the dataset\n",
    "    as_supervised=True,  # returns (image, label)\n",
    "    with_info=True,  # returns info about the dataset\n",
    ")\n",
    "\n",
    "# prepare index labels\n",
    "labels_index = info.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    # preprocess images\n",
    "    image = tf.reshape(image, (28, 28, 1))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    # preprocess labels\n",
    "    label = tf.one_hot(label, 10)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# we will create a function to prepare the dataset for training\n",
    "def dataset_prep(dataset):\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # shuffle the dataset\n",
    "    dataset = dataset.shuffle(1000)\n",
    "\n",
    "    # batch the dataset\n",
    "    dataset = dataset.batch(32)\n",
    "\n",
    "    # prefetch the dataset\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = dataset_prep(dataset[\"train\"])\n",
    "test_dataset = dataset_prep(dataset[\"test\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition <a name=\"model-definition\"></a>\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               401536    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 421,642\n",
      "Trainable params: 421,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, 3, padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loop <a name=\"custom-training-loop\"></a>\n",
    "[Back to top](#toc)\n",
    "\n",
    "Let's start applying the steps described above to create a custom training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define essential variables like the optimizer, loss function, and metrics that will be used to evaluate the model's performance.\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "train_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_metric = tf.keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the training and testing steps. These steps will be executed in the training loop.\n",
    "@tf.function  # this decorator will convert the function to a graph which will be executed in the GPU\n",
    "def train_step(images, labels):\n",
    "    # forward propagation starts\n",
    "    with tf.GradientTape() as tape:  # this will record all the operations performed inside the block\n",
    "        predictions = model(images, training=True)  # pass the images to the model\n",
    "        loss = loss_fn(labels, predictions)  # calculate the loss value\n",
    "\n",
    "    # forward propagation ends, now we will start the backward propagation\n",
    "\n",
    "    parameters = (\n",
    "        model.trainable_variables\n",
    "    )  # get all the trainable parameters of the model\n",
    "\n",
    "    gradients = tape.gradient(\n",
    "        loss, parameters\n",
    "    )  # calculate the gradients of the loss with respect to the parameters\n",
    "\n",
    "    gradients_parameters_tuple = zip(\n",
    "        gradients, parameters\n",
    "    )  # zip the gradients and weights together\n",
    "\n",
    "    optimizer.apply_gradients(\n",
    "        gradients_parameters_tuple\n",
    "    )  # apply the gradients to the weights\n",
    "\n",
    "    # backward propagation ends\n",
    "\n",
    "    # update the metrics using the labels and predictions\n",
    "    train_metric.update_state(labels, predictions)\n",
    "\n",
    "    # return the loss value to be used in the training loop\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Step 3: Define the testing step. This step will be executed in the training loop using the test dataset.\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images, training=True)  # pass the images to the model\n",
    "    loss = loss_fn(labels, predictions)  # calculate the loss value\n",
    "    test_metric.update_state(\n",
    "        labels, predictions\n",
    "    )  # update the metrics using the labels and predictions\n",
    "    return loss  # return the loss value to be used in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_trained = 0  # this will be used to keep track of the current epoch, this will be useful when we resume training from a checkpoint or when we want to train the model for more epochs\n",
    "epochs = 10  # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab61805412842638d795ab880ac217e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 09:37:18.123735: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 09:37:30.639854: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 09:37:32.155626: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Epoch 1 Ended\n",
      "  Train Loss: 0.3981, Train Metric: 0.8565 \n",
      "  Test Loss: 0.3074, Test Metric: 0.8903\n",
      "  \n",
      "\n",
      "  Epoch 2 Ended\n",
      "  Train Loss: 0.2633, Train Metric: 0.9047 \n",
      "  Test Loss: 0.2741, Test Metric: 0.9028\n",
      "  \n",
      "\n",
      "  Epoch 3 Ended\n",
      "  Train Loss: 0.2196, Train Metric: 0.9185 \n",
      "  Test Loss: 0.2465, Test Metric: 0.9103\n",
      "  \n",
      "\n",
      "  Epoch 4 Ended\n",
      "  Train Loss: 0.1864, Train Metric: 0.9314 \n",
      "  Test Loss: 0.2363, Test Metric: 0.9171\n",
      "  \n",
      "\n",
      "  Epoch 5 Ended\n",
      "  Train Loss: 0.1573, Train Metric: 0.9422 \n",
      "  Test Loss: 0.2470, Test Metric: 0.9162\n",
      "  \n",
      "\n",
      "  Epoch 6 Ended\n",
      "  Train Loss: 0.1340, Train Metric: 0.9503 \n",
      "  Test Loss: 0.2516, Test Metric: 0.9213\n",
      "  \n",
      "\n",
      "  Epoch 7 Ended\n",
      "  Train Loss: 0.1108, Train Metric: 0.9593 \n",
      "  Test Loss: 0.2548, Test Metric: 0.9195\n",
      "  \n",
      "\n",
      "  Epoch 8 Ended\n",
      "  Train Loss: 0.0898, Train Metric: 0.9665 \n",
      "  Test Loss: 0.2957, Test Metric: 0.9161\n",
      "  \n",
      "\n",
      "  Epoch 9 Ended\n",
      "  Train Loss: 0.0768, Train Metric: 0.9710 \n",
      "  Test Loss: 0.3073, Test Met